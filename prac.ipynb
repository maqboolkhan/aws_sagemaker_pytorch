{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caabeef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9d930f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker==2.68.0\n",
      "sagemaker-pyspark==1.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f798a7",
   "metadata": {},
   "source": [
    "### Creating bucket using Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ca57f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu-central-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "s3.Bucket(name='maq01-first-bucket')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_name = \"maq01-first-bucket\"\n",
    "s3 = boto3.resource(\"s3\")\n",
    "region = session.boto_region_name\n",
    "\n",
    "print(region)\n",
    "\n",
    "bucket_config = {\"LocationConstraint\": region}\n",
    "s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration=bucket_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921183ba",
   "metadata": {},
   "source": [
    "### Uploading the data to S3\n",
    "---\n",
    "\n",
    "Sagemaker expects our training, validation or testing data to be in a `S3` bucket! Hence, we have to upload our data into the bucket we just created!\n",
    "\n",
    "We have 2 ways!\n",
    "\n",
    "**Using Sagemaker's session object**\n",
    "\n",
    "https://sagemaker.readthedocs.io/en/stable/api/utility/session.html#sagemaker.session.Session.upload_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9bfcb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://maq01-first-bucket/data/Language Detection.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    We can also provide the target folder name where we want to upload, for that purpose we can use `key_prefix` parameter of upload_data method.\n",
    "    However, here we did not provide so default is \"data\" folder.\n",
    "'''\n",
    "\n",
    "inputs = session.upload_data(path=\"Language Detection.csv\", bucket=bucket_name)\n",
    "\n",
    "print(\"input spec (in this case, just an S3 path): {}\".format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8356fc7f",
   "metadata": {},
   "source": [
    "**Using Sagemaker's s3 utilities**\n",
    "\n",
    "\n",
    "https://sagemaker.readthedocs.io/en/stable/api/utility/s3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc0bdf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uploader = sagemaker.s3.S3Uploader()\n",
    "\n",
    "\n",
    "data_s3_uri = s3_uploader.upload(\n",
    "        local_path=\"Language Detection.csv\", desired_s3_uri=f\"s3://{bucket_name}/data\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a94cfa",
   "metadata": {},
   "source": [
    "## Training with Sagemaker using Pytorch\n",
    "---\n",
    "\n",
    "Amazon Sagemker provides so many functionalities to train Machine/Deep learning models. It also provides builtin algorthims so we dont even need to write our own algorithm (see [link](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html)). However, this notebook deals with how to train custom model using Pytorch and Sagemaker.\n",
    "\n",
    "In order to use Pytorch with Sagemaker, we have to use a `Pytorch` class provided by Sagemaker. Lets import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f43514c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f45467",
   "metadata": {},
   "source": [
    "This `Pytorch` model creates an estimator object which handles end-to-end training and deployment of custom PyTorch code. This estimator pull the Docker contrainer from Amazon ECR (Elastic Container Registry), and this container have `Python` and `Pytorch` pre-installed and then it start training our model in it. Once, training is finished it uploads artifacts including our model to `S3 bucket`. Hence, we only get charge of that training duration. After training, we can also deploy our model. Sagemaker can generate and deploy rest API endpoint with just one single line! We will see more in detail later.\n",
    "\n",
    "First, lets define our hypermeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de9f27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\": 10, \n",
    "    \"batch-size\": 64, \n",
    "    \"embedding-dim\": 125, \n",
    "    \"hidden-dim\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408fae1c",
   "metadata": {},
   "source": [
    "As we have already talked about estimator, in order to create an estimator we need to provide configuration. It requires version of Pytorch and Python as it will fetch the Docker container from ECR having these dependencies pre-installled.\n",
    "\n",
    "For more info about estimator config: https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5a0b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_config = {\n",
    "    \"source_dir\": \"scripts\", # we provide source_dir in order to install torchtext!!\n",
    "    \"entry_point\": \"train_script.py\",\n",
    "    \"framework_version\": \"1.9\", # Pytorch version\n",
    "    \"py_version\": \"py38\", # Python version\n",
    "    \"instance_type\": \"ml.m5.xlarge\", # type of docker container, i am using ml.m5.xlarge\n",
    "    \"instance_count\": 1,\n",
    "    \"role\": sagemaker.get_execution_role(),\n",
    "    \"output_path\": f\"s3://{bucket_name}\", # if this is not specified then SM will create new bucket to store artifacts\n",
    "    \"hyperparameters\": hyperparameters,\n",
    "    \"code_location\": f\"s3://{bucket_name}\" # This location is used when we deploy our endpoint if not specified another S3 bucket!\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84fbac",
   "metadata": {},
   "source": [
    "Everything is pretty self explanatory. Although, `source_dir` and `entry_point` is important to understand.\n",
    "\n",
    "As you have notice, we have a folder `scripts` and inside that folder we have our `entry_point`'s script. We dont need to create or have a folder for `entry_point` script but if you want to install custom dependencies using `requirements.txt` then you should create a folder and inside that folder put the `requirements.txt` and `entry_point`'s script. This is exactly what we did here as well, as we want to install `torchtext`!\n",
    "Sagemaker will automatically look for `requirements.txt` and install it.\n",
    "\n",
    "### Investigating train_script.py\n",
    "---\n",
    "\n",
    "**`main` block**:\n",
    "\n",
    "Most of the code in `train_script.py` is simply how you write your model using `Pytorch`. However, I would to explain few things here. Let's start with `main` block\n",
    "```Python\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # These variables are populate with estimator hypermeters\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=32)\n",
    "    parser.add_argument(\"--embedding-dim\", type=int, default=125)\n",
    "    parser.add_argument(\"--hidden-dim\", type=int, default=2)\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "```\n",
    "As we passed our `hyperparameters` in estimator config. Here you can see that `train_script.py` receives them as command-line options. \n",
    "\n",
    "Next  \n",
    "```Python\n",
    "    # Estimator's config \"output_path\" populates `SM_OUTPUT_DATA_DIR` enviroment variable\n",
    "    model_storage = os.environ['SM_OUTPUT_DATA_DIR']\n",
    "    \n",
    "    # https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#prepare-a-pytorch-training-script\n",
    "    # estimator.fit populates `SM_CHANNEL_TRAIN` enviroment variable\n",
    "    corpus_dir = os.environ['SM_CHANNEL_TRAIN']\n",
    "```\n",
    "Using estimator config, Sagemaker populates enviroment variables which we can access in `train_script.py`. Here you can see, enviroment variable of `SM_OUTPUT_DATA_DIR` was populate using `output_path` from estimator config. Same goes for `SM_CHANNEL_TRAIN` enviroment variable. However, it was populated using `pytorch_estimator.fit` (see next cells for more details).\n",
    "\n",
    "Now at the end of `main` block, notice I am not just storing `state_dict` our model as we going to need our vocabulary and its size in order to instantiate our model and to do inference.\n",
    "\n",
    "**`model_fn`, `input_fn`, `predict_fn`  functions**:\n",
    "\n",
    "As I mentioned earlier Sagemaker can also generate and deploy an endpoint for our model. We can use that end point for inference. In this notebook we are also going to deploy an endpoint. For deployment, we must need to provide `model_fn`, `input_fn`, `predict_fn` functions in our training scripts. Let's examine them\n",
    "\n",
    "**`model_fn`**:\n",
    "This function should have the following signature:\n",
    "\n",
    "```Python\n",
    "def model_fn(model_dir)\n",
    "```\n",
    "Sagemaker will inject `model_dir` and this is where we saved our model. This function should load the model from the `model_dir` and return it. You can return any kind of object but output of this function should contain model. The output of this function will be further used as an input to `predict_fn`\n",
    "\n",
    "**`input_fn`**:\n",
    "This function should have the following signature:\n",
    "\n",
    "```Python\n",
    "def input_fn(request_body, request_content_type)\n",
    "```\n",
    "Here we will recieve body of our endpoint. We can further process and validate the input here and then return the input which our model will use to do prediction. The output of this function will be further used as an input to `predict_fn`\n",
    "\n",
    "\n",
    "**`predict_fn`**:\n",
    "This function should have the following signature:\n",
    "\n",
    "```Python\n",
    "def predict_fn(input_fn_out, model_fn_out)\n",
    "```\n",
    "\n",
    "The first parameter is the output from `input_fn` and second one is output from the `predict_fn` function.\n",
    "\n",
    "That's it we are good to go with training and deployment.\n",
    "\n",
    "One can further read official docs of Sagemaker: https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d620380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(**estimator_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "557aec06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-05 21:08:38 Starting - Starting the training job...\n",
      "2021-12-05 21:09:01 Starting - Launching requested ML instancesProfilerReport-1638738518: InProgress\n",
      "...\n",
      "2021-12-05 21:09:29 Starting - Preparing the instances for training......\n",
      "2021-12-05 21:10:38 Downloading - Downloading input data...\n",
      "2021-12-05 21:11:01 Training - Downloading the training image...\n",
      "2021-12-05 21:11:33 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-12-05 21:11:34,462 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-12-05 21:11:34,463 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-12-05 21:11:34,471 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-12-05 21:11:40,706 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-12-05 21:11:40,977 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting torchtext\u001b[0m\n",
      "\u001b[34mDownloading torchtext-0.11.0-cp38-cp38-manylinux1_x86_64.whl (8.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from torchtext->-r requirements.txt (line 1)) (4.62.3)\u001b[0m\n",
      "\u001b[34mCollecting torch==1.10.0\u001b[0m\n",
      "\u001b[34mDownloading torch-1.10.0-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchtext->-r requirements.txt (line 1)) (2.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchtext->-r requirements.txt (line 1)) (1.21.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.10.0->torchtext->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext->-r requirements.txt (line 1)) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext->-r requirements.txt (line 1)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext->-r requirements.txt (line 1)) (1.26.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext->-r requirements.txt (line 1)) (2.0.4)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: torch, torchtext\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 1.9.1\u001b[0m\n",
      "\u001b[34mUninstalling torch-1.9.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-1.9.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed torch-1.10.0 torchtext-0.11.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2021-12-05 21:12:14,599 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-12-05 21:12:14,608 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-12-05 21:12:14,618 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-12-05 21:12:14,626 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 64,\n",
      "        \"embedding-dim\": 125,\n",
      "        \"hidden-dim\": 2,\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-12-05-21-08-38-151\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://maq01-first-bucket/pytorch-training-2021-12-05-21-08-38-151/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_script.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":64,\"embedding-dim\":125,\"epochs\":10,\"hidden-dim\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://maq01-first-bucket/pytorch-training-2021-12-05-21-08-38-151/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":64,\"embedding-dim\":125,\"epochs\":10,\"hidden-dim\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-12-05-21-08-38-151\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://maq01-first-bucket/pytorch-training-2021-12-05-21-08-38-151/source/sourcedir.tar.gz\",\"module_name\":\"train_script\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"64\",\"--embedding-dim\",\"125\",\"--epochs\",\"10\",\"--hidden-dim\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_EMBEDDING-DIM=125\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN-DIM=2\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train_script.py --batch-size 64 --embedding-dim 125 --epochs 10 --hidden-dim 2\u001b[0m\n",
      "\u001b[34mEpoch:  0\u001b[0m\n",
      "\u001b[34mTrain loss:  9.93921248857365\u001b[0m\n",
      "\u001b[34mEval loss:  9.852494451734755\u001b[0m\n",
      "\u001b[34mEpoch:  1\u001b[0m\n",
      "\u001b[34mTrain loss:  9.84232387986294\u001b[0m\n",
      "\u001b[34mEval loss:  9.77428256140815\u001b[0m\n",
      "\u001b[34mEpoch:  2\u001b[0m\n",
      "\u001b[34mTrain loss:  9.684471174728039\u001b[0m\n",
      "\u001b[34mEval loss:  9.581175910101997\u001b[0m\n",
      "\u001b[34mEpoch:  3\u001b[0m\n",
      "\u001b[34mTrain loss:  9.474894590156023\u001b[0m\n",
      "\u001b[34mEval loss:  9.351217163933647\u001b[0m\n",
      "\u001b[34mEpoch:  4\u001b[0m\n",
      "\u001b[34mTrain loss:  9.186787605285645\u001b[0m\n",
      "\u001b[34mEval loss:  8.994338565402561\u001b[0m\n",
      "\u001b[34mEpoch:  5\u001b[0m\n",
      "\u001b[34mTrain loss:  8.778174999148346\u001b[0m\n",
      "\u001b[34mEval loss:  8.612122959560818\u001b[0m\n",
      "\u001b[34mEpoch:  6\u001b[0m\n",
      "\u001b[34mTrain loss:  8.405691568241563\u001b[0m\n",
      "\u001b[34mEval loss:  8.261474079555935\u001b[0m\n",
      "\u001b[34mEpoch:  7\u001b[0m\n",
      "\u001b[34mTrain loss:  8.072958890781846\u001b[0m\n",
      "\u001b[34mEval loss:  7.944919904073079\u001b[0m\n",
      "\u001b[34mEpoch:  8\u001b[0m\n",
      "\u001b[34mTrain loss:  7.762938166773597\u001b[0m\n",
      "\u001b[34mEval loss:  7.686806943681505\u001b[0m\n",
      "\u001b[34mEpoch:  9\u001b[0m\n",
      "\u001b[34mTrain loss:  7.451325771420501\u001b[0m\n",
      "\u001b[34mEval loss:  7.388998084598118\u001b[0m\n",
      "\u001b[34m2021-12-05 21:12:37,789 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-12-05 21:13:02 Uploading - Uploading generated training model\n",
      "2021-12-05 21:13:02 Completed - Training job completed\n",
      "ProfilerReport-1638738518: NoIssuesFound\n",
      "Training seconds: 130\n",
      "Billable seconds: 130\n"
     ]
    }
   ],
   "source": [
    "# https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html\n",
    "data_channels = {\"train\": f\"s3://{bucket_name}/data/\"} # this will populate 'SM_CHANNEL_TRAIN' enviroment variable\n",
    "\n",
    "pytorch_estimator.fit(data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dd85b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://maq01-first-bucket/pytorch-training-2021-12-05-21-08-38-151/output/model.tar.gz'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7c75c",
   "metadata": {},
   "source": [
    "### The bug\n",
    "---\n",
    "\n",
    "At the time of creating this notebook (Dec 2021), there is a bug with Sagemaker API (2.68.0) that it returns incorrect path (check output of last cell)\n",
    "\n",
    "```\n",
    "s3://maq01-first-bucket/pytorch-training-2021-12-04-22-04-33-125/output/model.tar.gz\n",
    "```\n",
    "\n",
    "However, model is saved with `output.tar.gz`. I have filed the bug on Sagemaker Github repository.\n",
    "\n",
    "https://github.com/aws/sagemaker-python-sdk/issues/2762\n",
    "\n",
    "This bug causes to break the deployment of endpoint (see next cell). However, I also found workaround for it that renaming the `output.tar.gz` to `model.tar.gz` in `S3 bucket` will fix this problem!\n",
    "\n",
    "So if you running this notebook in Sagemaker, at this point your model has been uploaded to your specified bucket. Go to your `bucket` and then `output` folder and rename `output.tar.gz` to `model.tar.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44a45fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "predictor = pytorch_estimator.deploy(instance_type='ml.m5.xlarge',\n",
    "                                     initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "988ad144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pytorch-training-2021-12-05-21-28-43-864'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9420927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.serializers.NumpySerializer at 0x7fa898287a58>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.serializer # default is numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38459f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changine default serializer\n",
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53a952c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'French'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(\"how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dcfa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045f35d",
   "metadata": {},
   "source": [
    "References and courtesy: \n",
    "\n",
    "https://github.com/debnsuma/Intro-Transformer-BERT/blob/main/BERT-Disaster-Tweets-Prediction.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
